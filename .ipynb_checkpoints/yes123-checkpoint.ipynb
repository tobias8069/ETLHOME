{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'href'\n",
      "Total pages are 1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yes123_job_links.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-887d6886bd88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m#=============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yes123_job_links.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mopen_link\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mget_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yes123_job_links.csv'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ===========================================================================\n",
    "\n",
    "host = 'http://www.yes123.com.tw/admin/'\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(host + \"job_refer_list.asp\")\n",
    "\n",
    "# 輸入關鍵字搜尋\n",
    "browser.find_element_by_id('find_key1').clear()                       # 清除欄位文字\n",
    "browser.find_element_by_id('find_key1').send_keys('軟體')              # 輸入關鍵字\n",
    "time.sleep(2)\n",
    "browser.find_element_by_class_name('n_serch_btn').click()             # 按搜尋扭\n",
    "time.sleep(2)\n",
    "soup = BeautifulSoup(browser.page_source, 'lxml')                     # 所有網頁的原始碼\n",
    "time.sleep(2)\n",
    "counter = 0                                                           # 頁數\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        counter += 1                                                  # 頁數迴圈增加\n",
    "        jobs = []\n",
    "        for job in soup.select('a.jobname'):                          # 標題頁網頁連結\n",
    "            jobs.append(host + job['href'])                           \n",
    "\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.END)  \n",
    "        time.sleep(3)\n",
    "\n",
    "        with open('yes123_job_links.csv', 'a') as f:    # 寫入檔案\n",
    "            f.write('\\n'.join(jobs) + '\\n')             \n",
    "\n",
    "        browser.find_element_by_link_text('>').click()  # 點擊下一頁按鈕\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:                              \n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "print('Total pages are %d' % (counter))\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "#=====================================================================================\n",
    "wc = Counter()              \n",
    "wc[\"C\"] = 0               \n",
    "wc[\"C++\"] = 0\n",
    "wc[\"C#\"] = 0\n",
    "wc[\"PYTHON\"] = 0\n",
    "wc[\"JAVA\"] = 0\n",
    "wc[\"JAVASCRIPT\"] = 0\n",
    "wc[\"PHP\"] = 0\n",
    "wc[\"HTML\"] = 0\n",
    "wc[\"SQL\"] = 0\n",
    "wc[\"CSS\"] = 0\n",
    "wc[\"R\"] = 0\n",
    "wc[\"CSS\"] = 0\n",
    "wc[\"BASH\"] = 0\n",
    "wc[\"RUBY\"] = 0\n",
    "wc[\"PERL\"] = 0\n",
    "wc[\"SCALA\"] = 0\n",
    "wc[\"SWIFT\"] = 0\n",
    "wc[\"GO\"] = 0\n",
    "wc[\"DELPHI\"] = 0\n",
    "wc[\"TYPESCRIPT\"] = 0\n",
    "wc[\"JQUERY\"] = 0\n",
    "\n",
    "\n",
    "def get_inner(open_link):\n",
    "    try:\n",
    "        res = requests.get(open_link)\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        line1 = soup.select('div.comp_detail > ul > li > .rr')[0].text          # 徵才說明\n",
    "        line2 = str(soup.select('div.comp_detail > ul > li'))                   # 技能與求職專長\n",
    "        line = line1 + line2\n",
    "        up = line.upper()                                                       # 統一轉為大寫處理\n",
    "        line = re.findall('[A-Z]+[+#-C]*', \"%s\" % up)                           # 正規化處理\n",
    "        line_c = []                                                             \n",
    "        for line_a in line:                                                     \n",
    "            if line_a not in line_c:                                            \n",
    "                line_c.append(line_a)                                           \n",
    "\n",
    "        for language in line_c:                                                 \n",
    "            if language in wc:                                                  \n",
    "                wc[language] += 1                                              \n",
    "                                                       \n",
    "    except:\n",
    "        print(open_link)                                                        \n",
    "        pass\n",
    "\n",
    "#=============================================================================\n",
    "with open('yes123_job_links.csv', 'r') as file:              \n",
    "    for open_link in file.read().strip().split('\\n'):\n",
    "        get_inner(open_link)                                \n",
    "#=============================================================================\n",
    "language = OrderedDict(wc.most_common())\n",
    "print(language)\n",
    "\n",
    "with open('yes123.json', 'w') as fj:   \n",
    "    json.dump(language, fj)                 \n",
    "    fj.closed\n",
    "print(wc.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
